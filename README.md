# Spotify ETL ‚Üí Postgres ‚Üí FastAPI (Portfolio Project)

A tiny but complete data project that proves end-to-end skills:

**CSV ‚Üí clean/normalize ‚Üí Postgres ‚Üí API with filters/pagination ‚Üí tests ‚Üí Docker ‚Üí CI**

---

## ‚ú® What this does

- **ETL:** Loads a Spotify-like CSV (e.g., Kaggle) into Postgres with light cleaning.
- **API:** Serves tracks and simple analytics via **FastAPI** (with typed responses).
- **Filters & Pagination:** Query by artist, danceability, tempo range; sort & paginate.
- **Tests:** `pytest` smoke tests.
- **Dev UX:** `.env.example`, Makefile shortcuts, CORS enabled, typed schemas, OpenAPI docs.
- **Ops:** Dockerfile + docker-compose to spin up Postgres + API. GitHub Actions CI.

---

## üß± Tech Stack

- **Python 3.12**, **FastAPI**, **Uvicorn**
- **SQLAlchemy 2.x**, **psycopg**
- **PostgreSQL 16**
- **pandas** (for CSV ETL)
- **pytest**, **GitHub Actions** (CI)
- **Docker** + **docker-compose**

---

## üì¶ Repo Structure

.
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ api/
‚îÇ  ‚îÇ  ‚îú‚îÄ main.py        
‚îÇ  ‚îÇ  ‚îú‚îÄ tracks.py      
‚îÇ  ‚îÇ  ‚îî‚îÄ schemas.py     
‚îÇ  ‚îú‚îÄ db/
‚îÇ  ‚îÇ  ‚îú‚îÄ crud.py        
‚îÇ  ‚îÇ  ‚îú‚îÄ models.py      
‚îÇ  ‚îÇ  ‚îî‚îÄ session.py     
‚îÇ  ‚îî‚îÄ etl/
‚îÇ     ‚îî‚îÄ load_csv.py    
‚îú‚îÄ data/
‚îÇ  ‚îî‚îÄ raw/              
‚îú‚îÄ tests/
‚îÇ  ‚îî‚îÄ test_api.py       
‚îú‚îÄ .env.example         
‚îú‚îÄ requirements.txt
‚îú‚îÄ Dockerfile
‚îú‚îÄ docker-compose.yml
‚îú‚îÄ Makefile
‚îî‚îÄ README.md

---

## üöÄ Quickstart (Local)

> Prereqs: Python 3.12, Postgres (e.g., Homebrew), `psql`, and optional pgAdmin.

1. **Clone & setup Python**

```bash
git clone <your-repo-url>
cd spotify-etl-api
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
cp .env.example .env
```

brew services start postgresql@16

Create DB (only if it doesn't exist):

createdb spotify

3. (Optional) pgAdmin connection

‚Ä¢ Host: localhost, Port: 5432, User: postgres, Password: postgres, DB: spotify

4. Load sample data or a Kaggle CSV

‚Ä¢ Put your file at data/raw/spotify_kaggle.csv.
‚Ä¢ The loader auto-maps common columns and drops bad rows with missing names.

Avoid duplicates if you re-run

psql -h localhost -U postgres -d spotify -c "TRUNCATE TABLE tracks;"

Load CSV ‚Üí Postgres

python -m app.etl.load_csv data/raw/spotify_kaggle.csv

Expected output: multiple "Inserted rows ..." lines + final "Loaded N total rows"

5. Run the API
   uvicorn app.api.main:app --reload

or: make run

6. Open docs

‚Ä¢ Swagger UI: http://127.0.0.1:8000/docs
‚Ä¢ Health: http://127.0.0.1:8000/health

## üê≥ Quickstart (Docker Compose)

Spins up Postgres + API in one command.

docker compose up --build -d

App at http://127.0.0.1:8000/docs

Load data (pick one):
‚Ä¢ From inside the app container (CSV is copied with the repo):
docker compose exec app python -m app.etl.load_csv data/raw/spotify_kaggle.csv
‚Ä¢ From your host (connects to the mapped DB port):
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/spotify \
python -m app.etl.load_csv data/raw/spotify_kaggle.csv

Stop & clean:
docker compose down -v

## üîå Environment Variables

Create your .env from .env.example:

Local dev (host Postgres)

DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/spotify

Docker compose (uncomment if you run app inside compose)

DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/spotify

## üß™ Tests & CI

Run tests locally:
make test

or

pytest -q

CI (GitHub Actions):
‚Ä¢ .github/workflows/ci.yml boots Postgres service and runs pytest on push/PR.

If you see ModuleNotFoundError: app, add a pytest.ini with:
[pytest]
pythonpath = .

## üß≠ API Endpoints (FastAPI)

Base URL: http://127.0.0.1:8000
‚Ä¢ GET /health ‚Üí {"ok": true}

‚Ä¢ GET /api/stats/summary ‚Üí overall stats
Response:
{ "total_tracks": 12345, "avg_danceability": 0.62, "avg_tempo": 118.3 }

‚Ä¢ GET /api/stats/top-artists?limit=10 ‚Üí most frequent artists
Response: [{ "artist": "Artist", "count": 42 }, ...]

‚Ä¢ GET /api/tracks ‚Üí tracks with pagination and filters.
Query params:
‚Ä¢ limit (1‚Äì500, default 50)
‚Ä¢ offset (default 0)
‚Ä¢ q (free-text: track_name/artist/album, substring, case-insensitive)
‚Ä¢ artist (substring match on artist)
‚Ä¢ min_danceability (0‚Äì1)
‚Ä¢ tempo_min, tempo_max
‚Ä¢ sort (danceability|tempo|track_name)
‚Ä¢ order (asc|desc, default desc)
Response:
{
"items": [
{
"id": 123,
"track_name": "Song",
"artist": "Artist",
"album": "Album",
"danceability": 0.73,
"tempo": 120.01
}
],
"total": 421,
"next_offset": 50
}

Curl Examples:

Health

curl "http://127.0.0.1:8000/health"

Summary

curl "http://127.0.0.1:8000/api/stats/summary"

Top artists

curl "http://127.0.0.1:8000/api/stats/top-artists?limit=5"

First 5 tracks

curl "http://127.0.0.1:8000/api/tracks?limit=5"

Free-text search

curl "http://127.0.0.1:8000/api/tracks?limit=5&q=tame"

Artist substring

curl "http://127.0.0.1:8000/api/tracks?limit=5&artist=Quantic"

Highly danceable, sorted

curl "http://127.0.0.1:8000/api/tracks?limit=5&min_danceability=0.8&sort=danceability&order=desc"

Tempo range, sorted asc

curl "http://127.0.0.1:8000/api/tracks?limit=5&tempo_min=60&tempo_max=90&sort=tempo&order=asc"

Next page (use "next_offset" from previous response)

curl "http://127.0.0.1:8000/api/tracks?limit=5&offset=5"

## üßº CSV Loader Details

Path: app/etl/load_csv.py
‚Ä¢ Accepts common Spotify/Kaggle headers, e.g.:
track_name, artists, album_name, danceability, tempo
(Also handles name ‚Üí track_name, drops unnamed index columns.)
‚Ä¢ Cleans:
‚Ä¢ artists: takes the first artist when multiple; converts ; to ,
‚Ä¢ Coerces danceability and tempo to numeric
‚Ä¢ Drops rows with empty track_name or artist (DB has NOT NULL on these)
‚Ä¢ Chunked inserts: avoids giant SQL statements (500 row chunks)
‚Ä¢ Re-run safe if you TRUNCATE TABLE tracks; first

Load data:
psql -h localhost -U postgres -d spotify -c "TRUNCATE TABLE tracks;"
python -m app.etl.load_csv data/raw/spotify_kaggle.csv

## ‚ö° Performance (Optional)

Create indexes for faster queries on large datasets:
psql -h localhost -U postgres -d spotify -c "CREATE INDEX IF NOT EXISTS tracks_artist_idx ON tracks(artist);"
psql -h localhost -U postgres -d spotify -c "CREATE INDEX IF NOT EXISTS tracks_name_idx ON tracks(track_name);"

## üõ† Makefile Shortcuts

make run # uvicorn app.api.main:app --reload
make test # pytest -q
make docker-up # docker compose up --build -d
make docker-down # docker compose down -v
make load-sample # load small sample CSV (if present)

## üß© Troubleshooting

‚Ä¢ FATAL: role "postgres" does not exist
Create the role or use existing credentials; update DATABASE_URL accordingly.
‚Ä¢ Connection refused / port in use
Ensure Postgres is running on localhost:5432 (or match your .env).
‚Ä¢ Long SQL dump / IntegrityError
Caused by giant batch or null names. Fixed by chunked inserts + dropping rows with empty track_name/artist.
‚Ä¢ ModuleNotFoundError: app in tests
Ensure pytest.ini has:
[pytest]
pythonpath = .

    ‚Ä¢	Makefile ‚Äúmissing separator‚Äù

Make requires TABs, not spaces, before each command.
‚Ä¢ VS Code yellow squiggles on imports
Select interpreter: Cmd+Shift+P ‚Üí Python: Select Interpreter ‚Üí ./.venv/bin/python.

‚∏ª

## Screenshots

![FastAPI docs](docs/api-docs.png)

![Tracks endpoint example](docs/tracks-endpoint.png)

![Tracks row count](docs/tracks-count.png)


## üó∫Ô∏è Future Enhancements

‚Ä¢ artist_exact=true or word-boundary filter
‚Ä¢ More stats endpoints (tempo histograms, per-genre summaries)
‚Ä¢ Auth (API key / OAuth) and rate limiting
‚Ä¢ Frontend dashboard (React) consuming the API
‚Ä¢ Alembic migrations
